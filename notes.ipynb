{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e30596",
   "metadata": {},
   "source": [
    "# 信息论基础：编码长度与KL散度的关系\n",
    "\n",
    "## 1. 信息论的基本概念\n",
    "\n",
    "### 1.1 信息量的定义\n",
    "\n",
    "**核心问题**：如何量化信息？\n",
    "\n",
    "当我们观察到一个事件x时，获得的**信息量**定义为：\n",
    "$$I(x) = -\\log P(x)$$\n",
    "\n",
    "**直观理解**：\n",
    "- **概率越小**的事件 → **信息量越大**\n",
    "- **概率越大**的事件 → **信息量越小**\n",
    "\n",
    "**例子**：\n",
    "- \"太阳从东方升起\"：$P \\approx 1$，$I = -\\log 1 = 0$（没有信息）\n",
    "- \"明天地震\"：$P \\approx 0.001$，$I = -\\log 0.001 \\approx 10$ bits（信息量很大）\n",
    "\n",
    "### 1.2 香农编码定理\n",
    "\n",
    "**核心定理**：对于概率为$P(x)$的符号，最优编码长度为$-\\log P(x)$ bits\n",
    "\n",
    "**证明思路**：\n",
    "- 如果用$l(x)$ bits编码符号x，那么必须满足**Kraft不等式**：\n",
    "  $$\\sum_x 2^{-l(x)} \\leq 1$$\n",
    "- 在此约束下，最小化平均编码长度：\n",
    "  $$\\min \\sum_x P(x) l(x) \\quad \\text{subject to} \\quad \\sum_x 2^{-l(x)} \\leq 1$$\n",
    "- 使用拉格朗日乘数法，得到最优解：$l(x) = -\\log P(x)$\n",
    "\n",
    "## 2. 编码长度的详细分析\n",
    "\n",
    "### 2.1 最优编码（为P设计）\n",
    "\n",
    "假设信息源产生符合分布P的消息序列：\n",
    "\n",
    "**编码策略**：\n",
    "- 高概率符号 → 短编码\n",
    "- 低概率符号 → 长编码\n",
    "\n",
    "**编码长度**：每个符号$x$的编码长度为$-\\log P(x)$\n",
    "\n",
    "**平均编码长度**：\n",
    "$$L_P = \\sum_x P(x) \\cdot (-\\log P(x)) = -\\sum_x P(x) \\log P(x) = H(P)$$\n",
    "\n",
    "这就是**熵**！熵是最优编码的平均长度。\n",
    "\n",
    "### 2.2 次优编码（为Q设计，但用于P）\n",
    "\n",
    "**问题场景**：\n",
    "- 我们错误地认为消息符合分布Q\n",
    "- 基于Q设计了编码方案\n",
    "- 但实际消息符合分布P\n",
    "\n",
    "**编码长度**：每个符号$x$的编码长度为$-\\log Q(x)$\n",
    "\n",
    "**实际平均编码长度**：\n",
    "$$L_{P,Q} = \\sum_x P(x) \\cdot (-\\log Q(x)) = -\\sum_x P(x) \\log Q(x) = H(P,Q)$$\n",
    "\n",
    "这就是**交叉熵**！\n",
    "\n",
    "### 2.3 额外编码长度的推导\n",
    "\n",
    "**额外长度** = **次优编码长度** - **最优编码长度**\n",
    "\n",
    "$$\\text{额外长度} = L_{P,Q} - L_P$$\n",
    "\n",
    "$$= \\sum_x P(x)(-\\log Q(x)) - \\sum_x P(x)(-\\log P(x))$$\n",
    "\n",
    "$$= -\\sum_x P(x) \\log Q(x) + \\sum_x P(x) \\log P(x)$$\n",
    "\n",
    "$$= \\sum_x P(x) \\log P(x) - \\sum_x P(x) \\log Q(x)$$\n",
    "\n",
    "$$= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "$$= D_{KL}(P \\| Q)$$\n",
    "\n",
    "**结论**：KL散度就是使用错误编码方案导致的额外编码长度！\n",
    "\n",
    "## 3. 具体数值例子\n",
    "\n",
    "### 3.1 简单例子\n",
    "\n",
    "考虑一个信息源有3个符号：A, B, C\n",
    "\n",
    "**真实分布P**：\n",
    "- P(A) = 0.5，P(B) = 0.25，P(C) = 0.25\n",
    "\n",
    "**错误假设的分布Q**：\n",
    "- Q(A) = 0.25，Q(B) = 0.5，Q(C) = 0.25\n",
    "\n",
    "### 3.2 编码长度计算\n",
    "\n",
    "**最优编码（基于P）**：\n",
    "- A：$-\\log_2 0.5 = 1$ bit\n",
    "- B：$-\\log_2 0.25 = 2$ bits  \n",
    "- C：$-\\log_2 0.25 = 2$ bits\n",
    "\n",
    "**次优编码（基于Q）**：\n",
    "- A：$-\\log_2 0.25 = 2$ bits\n",
    "- B：$-\\log_2 0.5 = 1$ bit\n",
    "- C：$-\\log_2 0.25 = 2$ bits\n",
    "\n",
    "### 3.3 平均编码长度\n",
    "\n",
    "**最优平均长度**：\n",
    "$$L_P = 0.5 \\times 1 + 0.25 \\times 2 + 0.25 \\times 2 = 1.5 \\text{ bits}$$\n",
    "\n",
    "**次优平均长度**：\n",
    "$$L_{P,Q} = 0.5 \\times 2 + 0.25 \\times 1 + 0.25 \\times 2 = 1.75 \\text{ bits}$$\n",
    "\n",
    "**额外长度**：\n",
    "$$D_{KL}(P \\| Q) = 1.75 - 1.5 = 0.25 \\text{ bits}$$\n",
    "\n",
    "**验证**：\n",
    "$$D_{KL}(P \\| Q) = 0.5 \\log_2 \\frac{0.5}{0.25} + 0.25 \\log_2 \\frac{0.25}{0.5} + 0.25 \\log_2 \\frac{0.25}{0.25}$$\n",
    "$$= 0.5 \\times 1 + 0.25 \\times (-1) + 0.25 \\times 0 = 0.25 \\text{ bits}$$\n",
    "\n",
    "## 4. 编码的实际实现\n",
    "\n",
    "### 4.1 哈夫曼编码\n",
    "\n",
    "**最优前缀码**：实现接近理论下界的编码\n",
    "\n",
    "对于分布P = [0.5, 0.25, 0.25]：\n",
    "\n",
    "```\n",
    "符号  概率   哈夫曼码  长度\n",
    "A    0.5      0        1\n",
    "B    0.25     10       2  \n",
    "C    0.25     11       2\n",
    "```\n",
    "\n",
    "平均长度 = 0.5×1 + 0.25×2 + 0.25×2 = 1.5 bits（达到理论最优！）\n",
    "\n",
    "### 4.2 算术编码\n",
    "\n",
    "**更精确的方法**：可以达到熵的理论下界，即使对于单个符号也接近最优。\n",
    "\n",
    "### 4.3 现代应用\n",
    "\n",
    "**数据压缩**：\n",
    "- ZIP、RAR等压缩算法的理论基础\n",
    "- JPEG、MP3等有损压缩中的熵编码部分\n",
    "\n",
    "**通信系统**：\n",
    "- 信道编码\n",
    "- 错误纠正码\n",
    "\n",
    "## 5. 信息论在机器学习中的应用\n",
    "\n",
    "### 5.1 损失函数设计\n",
    "\n",
    "**交叉熵损失**：\n",
    "$$\\text{Loss} = -\\sum_i y_i \\log \\hat{y}_i = H(y, \\hat{y})$$\n",
    "\n",
    "其中$y$是真实分布，$\\hat{y}$是预测分布。\n",
    "\n",
    "**解释**：最小化交叉熵等价于最小化用预测分布编码真实分布的编码长度。\n",
    "\n",
    "### 5.2 模型选择\n",
    "\n",
    "**MDL原理（最小描述长度）**：\n",
    "$$\\text{最优模型} = \\arg\\min[\\text{模型复杂度} + \\text{数据编码长度}]$$\n",
    "\n",
    "### 5.3 特征选择\n",
    "\n",
    "**互信息**：\n",
    "$$I(X;Y) = D_{KL}(P(X,Y) \\| P(X)P(Y))$$\n",
    "\n",
    "衡量特征X和标签Y之间的依赖程度。\n",
    "\n",
    "## 6. 深层理解\n",
    "\n",
    "### 6.1 为什么是对数？\n",
    "\n",
    "**两个原因**：\n",
    "1. **可加性**：独立事件的信息量应该相加\n",
    "   $$I(x,y) = I(x) + I(y) \\Rightarrow -\\log P(x,y) = -\\log P(x) - \\log P(y)$$\n",
    "\n",
    "2. **单调性**：概率越小，信息量越大\n",
    "   $$P_1 < P_2 \\Rightarrow I(P_1) > I(P_2) \\Rightarrow -\\log P_1 > -\\log P_2$$\n",
    "\n",
    "### 6.2 不同底数的含义\n",
    "\n",
    "- **以2为底**：信息量单位为**bits**\n",
    "- **以e为底**：信息量单位为**nats**  \n",
    "- **以10为底**：信息量单位为**dits**\n",
    "\n",
    "在机器学习中通常使用自然对数（nats）。\n",
    "\n",
    "### 6.3 编码与压缩的关系\n",
    "\n",
    "**无损压缩的极限**：任何无损压缩算法都不能将数据压缩到小于其熵的大小。\n",
    "\n",
    "**香农的信息论告诉我们**：\n",
    "- 随机数据不可压缩\n",
    "- 有模式的数据可以压缩\n",
    "- 压缩比的理论上限由熵决定\n",
    "\n",
    "## 7. 总结\n",
    "\n",
    "### 7.1 核心洞察\n",
    "\n",
    "1. **信息量 = 负对数概率**：$I(x) = -\\log P(x)$\n",
    "2. **最优编码长度 = 信息量**：理论最优编码\n",
    "3. **KL散度 = 额外编码成本**：使用错误分布的代价\n",
    "4. **熵 = 最优平均编码长度**：信息的基本度量\n",
    "\n",
    "### 7.2 数学关系链\n",
    "\n",
    "$$\\boxed{\\text{概率} \\rightarrow \\text{信息量} \\rightarrow \\text{编码长度} \\rightarrow \\text{KL散度}}$$\n",
    "\n",
    "$$P(x) \\rightarrow -\\log P(x) \\rightarrow \\text{编码方案} \\rightarrow D_{KL}(P \\| Q)$$\n",
    "\n",
    "这个优美的理论框架连接了概率论、信息论和实际的编码应用，为现代机器学习提供了坚实的理论基础！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc22d26",
   "metadata": {},
   "source": [
    "# 为什么编码长度必须满足Kraft不等式？\n",
    "\n",
    "## 1. 问题的核心：唯一可译性\n",
    "\n",
    "### 1.1 基本要求\n",
    "\n",
    "任何有用的编码方案都必须满足**唯一可译性**：\n",
    "- 给定一串编码，只能有唯一的解码方式\n",
    "- 不能出现二义性\n",
    "\n",
    "**例子**：\n",
    "- ✅ 好的编码：{0, 10, 110, 111} → \"01011011\" 只能解码为 \"0-10-110-11\"\n",
    "- ❌ 坏的编码：{0, 01, 10} → \"010\" 可以解码为 \"0-10\" 或 \"01-0\"\n",
    "\n",
    "### 1.2 前缀码的概念\n",
    "\n",
    "**前缀码**是实现唯一可译性的最直接方法：\n",
    "- 任何码字都不是另一个码字的前缀\n",
    "- 前缀码自动保证唯一可译性\n",
    "\n",
    "## 2. 二叉树表示法\n",
    "\n",
    "### 2.1 码字与二叉树的对应\n",
    "\n",
    "每个前缀码都可以用**满二叉树**表示：\n",
    "- 每个内部节点有两个分支：0（左）和1（右）\n",
    "- 每个码字对应一个叶子节点\n",
    "- 从根到叶子的路径就是码字\n",
    "\n",
    "**例子**：码字 {0, 10, 110, 111}\n",
    "\n",
    "```\n",
    "        根\n",
    "       / \\\n",
    "      0   1\n",
    "     (A) / \\\n",
    "        0   1\n",
    "       (B) / \\\n",
    "          0   1\n",
    "         (C) (D)\n",
    "```\n",
    "\n",
    "- A: \"0\" (长度1)\n",
    "- B: \"10\" (长度2)  \n",
    "- C: \"110\" (长度3)\n",
    "- D: \"111\" (长度3)\n",
    "\n",
    "### 2.2 二叉树的约束\n",
    "\n",
    "**关键约束**：在二叉树中，如果一个节点是叶子节点（对应一个码字），那么它的所有子节点都不能再被使用。\n",
    "\n",
    "这个约束直接导致了Kraft不等式！\n",
    "\n",
    "## 3. Kraft不等式的直观推导\n",
    "\n",
    "### 3.1 \"面积\"类比\n",
    "\n",
    "把整个二叉树看作一个**单位正方形**：\n",
    "\n",
    "**深度1**：可以分成2个位置，每个位置占\"面积\" $\\frac{1}{2} = 2^{-1}$\n",
    "\n",
    "**深度2**：可以分成4个位置，每个位置占\"面积\" $\\frac{1}{4} = 2^{-2}$\n",
    "\n",
    "**深度d**：可以分成$2^d$个位置，每个位置占\"面积\" $\\frac{1}{2^d} = 2^{-d}$\n",
    "\n",
    "### 3.2 码字占用的\"面积\"\n",
    "\n",
    "如果一个码字的长度为$l$，它在深度$l$处占用一个叶子节点，那么：\n",
    "- 它\"占用\"的面积为 $2^{-l}$\n",
    "- 更重要的是，它**阻止**了其所有子节点被使用\n",
    "\n",
    "**例子**：长度为2的码字\"10\"\n",
    "- 直接占用深度2的一个位置：面积 $2^{-2} = 0.25$\n",
    "- 阻止了\"100\"和\"101\"被使用（如果允许的话）\n",
    "\n",
    "### 3.3 不等式的推导\n",
    "\n",
    "所有码字占用的总\"面积\"不能超过1：\n",
    "$$\\sum_{\\text{所有码字} x} 2^{-l(x)} \\leq 1$$\n",
    "\n",
    "这就是**Kraft不等式**！\n",
    "\n",
    "## 4. 严格的数学证明\n",
    "\n",
    "### 4.1 必要性证明（归纳法）\n",
    "\n",
    "**定理**：如果存在前缀码，则必须满足Kraft不等式。\n",
    "\n",
    "**证明**：\n",
    "设有$n$个码字，长度分别为$l_1 \\leq l_2 \\leq \\cdots \\leq l_n$。\n",
    "\n",
    "**基础步骤**：$n=1$时，$2^{-l_1} \\leq 1$显然成立。\n",
    "\n",
    "**归纳步骤**：假设对$k$个码字成立，考虑第$k+1$个码字。\n",
    "\n",
    "- 前$k$个码字占用\"面积\"$S_k = \\sum_{i=1}^k 2^{-l_i} \\leq 1$\n",
    "- 第$k+1$个码字长度为$l_{k+1}$，需要面积$2^{-l_{k+1}}$\n",
    "- 由于前缀码的约束，剩余可用面积为$1 - S_k$\n",
    "- 必须有$2^{-l_{k+1}} \\leq 1 - S_k$\n",
    "- 因此$S_{k+1} = S_k + 2^{-l_{k+1}} \\leq 1$\n",
    "\n",
    "### 4.2 充分性证明（构造法）\n",
    "\n",
    "**定理**：如果满足Kraft不等式，则存在对应的前缀码。\n",
    "\n",
    "**证明**：直接构造算法\n",
    "\n",
    "1. **排序**：将码字长度排序 $l_1 \\leq l_2 \\leq \\cdots \\leq l_n$\n",
    "\n",
    "2. **贪心分配**：\n",
    "   ```\n",
    "   available = 0  # 当前可用的最小编码\n",
    "   for i = 1 to n:\n",
    "       分配码字 i 为 binary(available, l_i)\n",
    "       available = available + 1\n",
    "       if i < n and l_{i+1} > l_i:\n",
    "           available = available << (l_{i+1} - l_i)\n",
    "   ```\n",
    "\n",
    "3. **正确性保证**：Kraft不等式确保这个过程不会\"溢出\"\n",
    "\n",
    "## 5. 为什么是 $2^{-l(x)}$？\n",
    "\n",
    "### 5.1 二进制的自然性\n",
    "\n",
    "在二进制编码中：\n",
    "- 深度$l$处最多有$2^l$个可能位置\n",
    "- 每个位置占用总数的比例为$\\frac{1}{2^l} = 2^{-l}$\n",
    "\n",
    "### 5.2 一般化：D进制码\n",
    "\n",
    "对于D进制编码，Kraft不等式变为：\n",
    "$$\\sum_x D^{-l(x)} \\leq 1$$\n",
    "\n",
    "这说明Kraft不等式是编码约束的普遍规律。\n",
    "\n",
    "## 6. 实际例子验证\n",
    "\n",
    "### 6.1 满足Kraft不等式的例子\n",
    "\n",
    "**码字长度**：[1, 2, 3, 3]\n",
    "\n",
    "**验证**：\n",
    "$$2^{-1} + 2^{-2} + 2^{-3} + 2^{-3} = 0.5 + 0.25 + 0.125 + 0.125 = 1.0$$\n",
    "\n",
    "**构造**：\n",
    "- 长度1：分配\"0\"\n",
    "- 长度2：分配\"10\"  \n",
    "- 长度3：分配\"110\"\n",
    "- 长度3：分配\"111\"\n",
    "\n",
    "**二叉树**：\n",
    "```\n",
    "     根\n",
    "    / \\\n",
    "   0   1\n",
    "  (A) / \\\n",
    "     0   1\n",
    "    (B) / \\\n",
    "       0   1\n",
    "      (C) (D)\n",
    "```\n",
    "\n",
    "### 6.2 违反Kraft不等式的例子\n",
    "\n",
    "**码字长度**：[1, 1, 2]\n",
    "\n",
    "**验证**：\n",
    "$$2^{-1} + 2^{-1} + 2^{-2} = 0.5 + 0.5 + 0.25 = 1.25 > 1$$\n",
    "\n",
    "**问题**：两个长度为1的码字已经用完了深度1的所有位置（\"0\"和\"1\"），无法再添加任何码字！\n",
    "\n",
    "## 7. 深层含义\n",
    "\n",
    "### 7.1 信息论的约束\n",
    "\n",
    "Kraft不等式揭示了编码的**根本限制**：\n",
    "- 短码字是稀缺资源\n",
    "- 使用短码字会\"消耗\"编码空间\n",
    "- 必须在码字长度之间做出权衡\n",
    "\n",
    "### 7.2 与概率的深刻联系\n",
    "\n",
    "令$q_x = 2^{-l(x)}$，则Kraft不等式变为：\n",
    "$$\\sum_x q_x \\leq 1$$\n",
    "\n",
    "这意味着$\\{q_x\\}$可以看作某种\"伪概率分布\"！\n",
    "\n",
    "实际上，这正是**均匀分布假设**下的最优编码长度对应的概率。\n",
    "\n",
    "## 总结\n",
    "\n",
    "**Kraft不等式的本质**：\n",
    "1. **几何约束**：二叉树结构的自然限制\n",
    "2. **唯一性保证**：确保编码可以唯一解码\n",
    "3. **资源分配**：短码字消耗宝贵的编码空间\n",
    "4. **概率联系**：连接编码长度与概率分布\n",
    "\n",
    "这个优美的不等式是信息论的基石，它告诉我们**什么是可能的编码方案**！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16fcfe",
   "metadata": {},
   "source": [
    "# McMillan定理：扩展到所有唯一可译码\n",
    "\n",
    "## 1. 问题的提出\n",
    "\n",
    "### 1.1 前缀码的局限性\n",
    "\n",
    "前面的证明确实只适用于**前缀码**，但是：\n",
    "- 前缀码只是唯一可译码的一个**子集**\n",
    "- 存在其他类型的唯一可译码，它们不是前缀码\n",
    "\n",
    "**例子**：码字集合 {0, 10, 011}\n",
    "- ❌ **不是前缀码**：没有任何码字是其他码字的前缀\n",
    "- ✅ **是唯一可译码**：任何码字序列都可以唯一解码\n",
    "\n",
    "**验证唯一可译性**：\n",
    "- \"010\" → 只能解码为 \"0-10\"（\"01-0\"不可能，因为\"01\"不是码字）\n",
    "- \"01011\" → 只能解码为 \"0-10-11\"\n",
    "\n",
    "### 1.2 McMillan定理的表述\n",
    "\n",
    "**McMillan定理**（1956年）：\n",
    "> 对于任何**唯一可译码**，其码字长度必须满足Kraft不等式：\n",
    "> $$\\sum_{i=1}^n 2^{-l_i} \\leq 1$$\n",
    "\n",
    "这个定理比Kraft不等式更强，因为它适用于**所有**唯一可译码，不仅仅是前缀码。\n",
    "\n",
    "## 2. McMillan定理的证明思路\n",
    "\n",
    "### 2.1 核心思想\n",
    "\n",
    "**关键洞察**：即使不是前缀码，唯一可译性也会对码字长度产生**约束**。\n",
    "\n",
    "**证明策略**：\n",
    "1. 考虑所有可能的$n$个码字的**连接序列**\n",
    "2. 分析这些序列的**长度分布**\n",
    "3. 利用唯一可译性导出约束条件\n",
    "\n",
    "### 2.2 数学证明\n",
    "\n",
    "**设置**：\n",
    "- 码字集合：$C = \\{c_1, c_2, \\ldots, c_k\\}$\n",
    "- 码字长度：$l_1, l_2, \\ldots, l_k$\n",
    "- 定义：$K = \\sum_{i=1}^k 2^{-l_i}$\n",
    "\n",
    "**目标**：证明 $K \\leq 1$\n",
    "\n",
    "**Step 1：考虑$n$个码字的连接**\n",
    "\n",
    "考虑所有可能的$n$个码字的连接序列：\n",
    "$$c_{i_1} c_{i_2} \\cdots c_{i_n}$$\n",
    "\n",
    "其中每个$i_j \\in \\{1, 2, \\ldots, k\\}$。\n",
    "\n",
    "总共有$k^n$种这样的序列。\n",
    "\n",
    "**Step 2：按长度分组**\n",
    "\n",
    "设$N_m$为长度恰好为$m$的连接序列数量。\n",
    "\n",
    "长度为$m$的连接序列的最小长度：$n \\cdot l_{\\min}$\n",
    "长度为$m$的连接序列的最大长度：$n \\cdot l_{\\max}$\n",
    "\n",
    "**Step 3：关键等式**\n",
    "\n",
    "所有连接序列的总数：\n",
    "$$\\sum_m N_m = k^n$$\n",
    "\n",
    "同时，$K^n$的展开：\n",
    "$$K^n = \\left(\\sum_{i=1}^k 2^{-l_i}\\right)^n = \\sum_{所有连接序列} 2^{-(\\text{序列总长度})}$$\n",
    "\n",
    "按长度分组：\n",
    "$$K^n = \\sum_m N_m \\cdot 2^{-m}$$\n",
    "\n",
    "**Step 4：唯一可译性的约束**\n",
    "\n",
    "**关键观察**：由于唯一可译性，长度为$m$的**不同二进制序列**最多有$2^m$个。\n",
    "\n",
    "因此：$N_m \\leq 2^m$\n",
    "\n",
    "**Step 5：导出不等式**\n",
    "\n",
    "结合上述结果：\n",
    "$$K^n = \\sum_m N_m \\cdot 2^{-m} \\leq \\sum_m 2^m \\cdot 2^{-m} = \\sum_m 1$$\n",
    "\n",
    "但求和范围是有限的（从$n \\cdot l_{\\min}$到$n \\cdot l_{\\max}$），所以：\n",
    "$$K^n \\leq n \\cdot (l_{\\max} - l_{\\min} + 1)$$\n",
    "\n",
    "**Step 6：取极限**\n",
    "\n",
    "当$n \\to \\infty$时：\n",
    "- 如果$K > 1$，则$K^n \\to \\infty$\n",
    "- 但右边是线性增长\n",
    "\n",
    "这导致矛盾，因此必须有$K \\leq 1$。\n",
    "\n",
    "## 3. 更严格的证明\n",
    "\n",
    "### 3.1 改进的论证\n",
    "\n",
    "**问题**：上面的证明在极限步骤不够严格。\n",
    "\n",
    "**改进思路**：使用**Sardinas-Patterson算法**的思想。\n",
    "\n",
    "**核心思想**：如果$K > 1$，则必然存在两个不同的码字序列对应同一个二进制串，违反唯一可译性。\n",
    "\n",
    "### 3.2 构造性证明\n",
    "\n",
    "**假设**：$K = \\sum_{i=1}^k 2^{-l_i} > 1$\n",
    "\n",
    "**目标**：构造一个反例证明这会导致歧义。\n",
    "\n",
    "**Step 1：选择合适的$n$**\n",
    "\n",
    "选择足够大的$n$，使得：\n",
    "$$K^n > 2^{n \\cdot l_{\\min}}$$\n",
    "\n",
    "**Step 2：鸽笼原理**\n",
    "\n",
    "- 总共有$k^n$个码字连接序列\n",
    "- 长度在$[n \\cdot l_{\\min}, n \\cdot l_{\\max}]$范围内的二进制串总数有限\n",
    "- 由于$K^n$的增长超过了可用二进制串的数量，必然存在冲突\n",
    "\n",
    "**Step 3：矛盾**\n",
    "\n",
    "这意味着至少存在两个不同的码字序列对应同一个二进制串，违反了唯一可译性。\n",
    "\n",
    "## 4. 直观理解\n",
    "\n",
    "### 4.1 为什么非前缀码也受约束？\n",
    "\n",
    "**关键洞察**：虽然非前缀码允许一个码字是另一个的前缀，但**唯一可译性**仍然限制了可能的组合。\n",
    "\n",
    "**例子分析**：{0, 10, 011}\n",
    "- Kraft和：$2^{-1} + 2^{-2} + 2^{-3} = 0.5 + 0.25 + 0.125 = 0.875 < 1$ ✅\n",
    "- 虽然不是前缀码，但仍然满足约束\n",
    "\n",
    "### 4.2 \"编码空间\"的消耗\n",
    "\n",
    "即使不是前缀码，每个码字仍然\"占用\"一定的编码空间：\n",
    "- 短码字仍然是稀缺资源\n",
    "- 唯一可译性确保不能有太多短码字\n",
    "\n",
    "## 5. 实际例子\n",
    "\n",
    "### 5.1 例子1：满足McMillan定理的非前缀码\n",
    "\n",
    "**码字**：{0, 10, 110, 1110, 1111}\n",
    "\n",
    "**检查前缀性**：\n",
    "- \"1\"是\"10\"、\"110\"、\"1110\"、\"1111\"的前缀 ❌ 不是前缀码\n",
    "\n",
    "**检查唯一可译性**：\n",
    "- 任何序列都可以唯一解码（从左到右贪婪匹配）✅\n",
    "\n",
    "**验证Kraft不等式**：\n",
    "$$2^{-1} + 2^{-2} + 2^{-3} + 2^{-4} + 2^{-4} = 0.5 + 0.25 + 0.125 + 0.0625 + 0.0625 = 1.0$$\n",
    "\n",
    "### 5.2 例子2：违反McMillan定理的码\n",
    "\n",
    "**码字**：{1, 10, 100, 1000}\n",
    "\n",
    "**Kraft和**：\n",
    "$$2^{-1} + 2^{-2} + 2^{-3} + 2^{-4} = 0.5 + 0.25 + 0.125 + 0.0625 = 0.9375 < 1$$\n",
    "\n",
    "但是这个码**不是唯一可译的**！\n",
    "\n",
    "**问题**：\"1000\"可以解码为：\n",
    "- \"1-0-0-0\"（如果\"0\"是码字）\n",
    "- \"10-0-0\"（如果\"0\"是码字）\n",
    "- \"100-0\"（如果\"0\"是码字）\n",
    "- \"1000\"\n",
    "\n",
    "由于\"0\"不在码字集合中，只能解码为\"1000\"，所以实际上是唯一可译的。\n",
    "\n",
    "让我们换个例子：{0, 1, 01, 10}\n",
    "\n",
    "**Kraft和**：$2^{-1} + 2^{-1} + 2^{-2} + 2^{-2} = 1.5 > 1$\n",
    "\n",
    "**唯一可译性检查**：\"01\"可以解码为\"0-1\"或\"01\" ❌\n",
    "\n",
    "## 6. 重要意义\n",
    "\n",
    "### 6.1 编码理论的完整性\n",
    "\n",
    "McMillan定理完成了编码理论的基础：\n",
    "- **Kraft不等式**：前缀码存在的充要条件\n",
    "- **McMillan定理**：所有唯一可译码的必要条件\n",
    "\n",
    "### 6.2 实际应用\n",
    "\n",
    "1. **编码设计**：任何实用编码都必须满足这个约束\n",
    "2. **压缩极限**：为数据压缩提供理论界限\n",
    "3. **算法复杂度**：编码解码算法的复杂度分析\n",
    "\n",
    "## 总结\n",
    "\n",
    "**McMillan定理的意义**：\n",
    "1. **普遍性**：适用于所有唯一可译码，不仅仅是前缀码\n",
    "2. **必要性**：这是所有实用编码必须满足的基本约束\n",
    "3. **理论完备性**：完善了信息论编码理论的基础\n",
    "\n",
    "**证明的核心思想**：\n",
    "- 利用唯一可译性对码字组合的约束\n",
    "- 通过计数论证和鸽笼原理\n",
    "- 证明违反Kraft不等式必然导致歧义\n",
    "\n",
    "这个优美的定理告诉我们：无论采用什么编码策略，只要要求唯一可译，就必须遵守Kraft不等式这一基本规律！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d512b",
   "metadata": {},
   "source": [
    "# KL散度（Kullback-Leibler Divergence）详解\n",
    "\n",
    "## 1. 什么是KL散度？\n",
    "\n",
    "### 1.1 定义\n",
    "\n",
    "KL散度，也称为**相对熵**，是衡量两个概率分布之间\"差异\"的一种度量。对于两个概率分布P和Q，从P到Q的KL散度定义为：\n",
    "\n",
    "**离散分布**：\n",
    "$$D_{KL}(P \\| Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "**连续分布**：\n",
    "$$D_{KL}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx$$\n",
    "\n",
    "### 1.2 直观理解\n",
    "\n",
    "KL散度可以理解为：\n",
    "- **信息损失**：用分布Q来近似分布P时损失了多少信息\n",
    "- **额外编码长度**：用为Q设计的编码来编码P的样本需要多少额外比特\n",
    "- **\"惊讶程度\"**：当期望是Q但观察到P时的平均惊讶程度\n",
    "\n",
    "## 2. 数学性质\n",
    "\n",
    "### 2.1 非负性\n",
    "\n",
    "**重要性质**：$D_{KL}(P \\| Q) \\geq 0$，当且仅当 $P = Q$ 时等于0\n",
    "\n",
    "**证明**：使用Jensen不等式\n",
    "$$D_{KL}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} = -\\sum_x P(x) \\log \\frac{Q(x)}{P(x)}$$\n",
    "\n",
    "由于 $\\log$ 是凹函数，Jensen不等式给出：\n",
    "$$-\\sum_x P(x) \\log \\frac{Q(x)}{P(x)} \\geq -\\log \\sum_x P(x) \\frac{Q(x)}{P(x)} = -\\log \\sum_x Q(x) = 0$$\n",
    "\n",
    "### 2.2 非对称性\n",
    "\n",
    "**关键特点**：$D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$\n",
    "\n",
    "KL散度不是真正的\"距离\"，因为它：\n",
    "- ✅ 满足非负性\n",
    "- ✅ 满足恒等性：$D_{KL}(P \\| P) = 0$\n",
    "- ❌ 不满足对称性：$D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$\n",
    "- ❌ 不满足三角不等式\n",
    "\n",
    "### 2.3 与其他概念的关系\n",
    "\n",
    "**与交叉熵的关系**：\n",
    "$$D_{KL}(P \\| Q) = H(P, Q) - H(P)$$\n",
    "\n",
    "其中：\n",
    "- $H(P, Q) = -\\sum_x P(x) \\log Q(x)$ 是交叉熵\n",
    "- $H(P) = -\\sum_x P(x) \\log P(x)$ 是熵\n",
    "\n",
    "## 3. 信息论解释\n",
    "\n",
    "### 3.1 信息编码视角\n",
    "\n",
    "假设我们有一个信息源产生符合分布P的消息：\n",
    "\n",
    "**最优编码**：为P设计的编码，每个符号x的编码长度为 $-\\log P(x)$\n",
    "**次优编码**：为Q设计的编码，每个符号x的编码长度为 $-\\log Q(x)$\n",
    "\n",
    "**额外编码长度**：\n",
    "$$\\text{额外长度} = \\sum_x P(x)(-\\log Q(x)) - \\sum_x P(x)(-\\log P(x))$$\n",
    "$$= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} = D_{KL}(P \\| Q)$$\n",
    "\n",
    "### 3.2 似然比检验\n",
    "\n",
    "KL散度也可以理解为**对数似然比的期望**：\n",
    "$$D_{KL}(P \\| Q) = E_{x \\sim P}\\left[\\log \\frac{P(x)}{Q(x)}\\right]$$\n",
    "\n",
    "这在统计假设检验中有重要应用。\n",
    "\n",
    "## 4. 在机器学习中的应用\n",
    "\n",
    "### 4.1 损失函数\n",
    "\n",
    "在分类问题中，KL散度常用作损失函数：\n",
    "$$\\text{Loss} = D_{KL}(y_{\\text{true}} \\| y_{\\text{pred}})$$\n",
    "\n",
    "其中 $y_{\\text{true}}$ 是真实标签的one-hot编码，$y_{\\text{pred}}$ 是模型预测的概率分布。\n",
    "\n",
    "### 4.2 变分推断\n",
    "\n",
    "在变分贝叶斯中，我们最小化：\n",
    "$$D_{KL}(q(z) \\| p(z|x))$$\n",
    "\n",
    "寻找变分分布 $q(z)$ 来近似后验分布 $p(z|x)$。\n",
    "\n",
    "### 4.3 生成模型\n",
    "\n",
    "在VAE中，KL散度用于正则化：\n",
    "$$\\text{Loss} = \\text{重构损失} + \\beta \\cdot D_{KL}(q(z|x) \\| p(z))$$\n",
    "\n",
    "确保编码器学到的潜在分布接近先验分布。\n",
    "\n",
    "## 5. 计算示例\n",
    "\n",
    "### 5.1 简单例子\n",
    "\n",
    "考虑两个硬币：\n",
    "- **硬币A**：$P = [0.5, 0.5]$（公平硬币）\n",
    "- **硬币B**：$Q = [0.1, 0.9]$（有偏硬币）\n",
    "\n",
    "计算 $D_{KL}(P \\| Q)$：\n",
    "$$D_{KL}(P \\| Q) = 0.5 \\log \\frac{0.5}{0.1} + 0.5 \\log \\frac{0.5}{0.9}$$\n",
    "$$= 0.5 \\log 5 + 0.5 \\log \\frac{5}{9} = 0.5 \\times 1.609 + 0.5 \\times (-0.588) = 0.511$$\n",
    "\n",
    "计算 $D_{KL}(Q \\| P)$：\n",
    "$$D_{KL}(Q \\| P) = 0.1 \\log \\frac{0.1}{0.5} + 0.9 \\log \\frac{0.9}{0.5}$$\n",
    "$$= 0.1 \\log 0.2 + 0.9 \\log 1.8 = 0.1 \\times (-1.609) + 0.9 \\times 0.588 = 0.368$$\n",
    "\n",
    "注意：$D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$\n",
    "\n",
    "### 5.2 连续分布例子\n",
    "\n",
    "两个正态分布：\n",
    "- $P = \\mathcal{N}(\\mu_1, \\sigma_1^2)$\n",
    "- $Q = \\mathcal{N}(\\mu_2, \\sigma_2^2)$\n",
    "\n",
    "$$D_{KL}(P \\| Q) = \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$\n",
    "\n",
    "## 6. 实际应用注意事项\n",
    "\n",
    "### 6.1 数值稳定性\n",
    "\n",
    "当 $Q(x) = 0$ 但 $P(x) > 0$ 时，$\\log \\frac{P(x)}{Q(x)} = +\\infty$\n",
    "\n",
    "**解决方案**：\n",
    "- 添加小的正数：$Q(x) + \\epsilon$\n",
    "- 使用平滑技术\n",
    "- 确保支撑集匹配：$\\text{support}(P) \\subseteq \\text{support}(Q)$\n",
    "\n",
    "### 6.2 计算技巧\n",
    "\n",
    "对于深度学习：\n",
    "```python\n",
    "# 数值稳定的KL散度计算\n",
    "kl_div = torch.nn.functional.kl_div(\n",
    "    log_probs_q,  # log Q(x)\n",
    "    probs_p,      # P(x)\n",
    "    reduction='batchmean'\n",
    ")\n",
    "```\n",
    "\n",
    "### 6.3 归一化\n",
    "\n",
    "确保P和Q都是合法的概率分布：\n",
    "- 非负性：$P(x) \\geq 0, Q(x) \\geq 0$\n",
    "- 归一化：$\\sum_x P(x) = 1, \\sum_x Q(x) = 1$\n",
    "\n",
    "## 7. 相关概念\n",
    "\n",
    "### 7.1 JS散度（Jensen-Shannon Divergence）\n",
    "\n",
    "对称的版本：\n",
    "$$JS(P, Q) = \\frac{1}{2}D_{KL}(P \\| M) + \\frac{1}{2}D_{KL}(Q \\| M)$$\n",
    "\n",
    "其中 $M = \\frac{1}{2}(P + Q)$\n",
    "\n",
    "### 7.2 Wasserstein距离\n",
    "\n",
    "另一种度量分布差异的方法，在GAN中广泛使用。\n",
    "\n",
    "### 7.3 χ²散度\n",
    "\n",
    "$$\\chi^2(P \\| Q) = \\sum_x \\frac{(P(x) - Q(x))^2}{Q(x)}$$\n",
    "\n",
    "## 总结\n",
    "\n",
    "KL散度是一个基础而重要的概念：\n",
    "\n",
    "1. **数学性质**：非负、非对称、恒等性\n",
    "2. **信息论意义**：信息损失、编码长度差异\n",
    "3. **机器学习应用**：损失函数、变分推断、生成模型\n",
    "4. **计算注意事项**：数值稳定性、支撑集匹配\n",
    "\n",
    "在自然梯度下降的上下文中，KL散度被用来定义统计流形上的度量，这正是Fisher信息矩阵出现的原因！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb6120",
   "metadata": {},
   "source": [
    "# 自然梯度下降：为什么使用Fisher信息矩阵？\n",
    "\n",
    "## 1. 问题的根本：参数空间的几何结构\n",
    "\n",
    "### 1.1 传统梯度下降的局限性\n",
    "\n",
    "**传统梯度下降假设**：\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta L(\\theta_t)$$\n",
    "\n",
    "这个公式隐含地假设：\n",
    "- 参数空间是**平坦的欧几里得空间**\n",
    "- 所有参数方向的\"重要性\"相同\n",
    "- 使用欧几里得距离：$\\|d\\theta\\|^2 = \\sum_i (d\\theta_i)^2$\n",
    "\n",
    "### 1.2 现实中的问题\n",
    "\n",
    "在机器学习中，我们处理的是**概率分布族**：\n",
    "$$\\mathcal{P} = \\{p(x|\\theta) : \\theta \\in \\Theta\\}$$\n",
    "\n",
    "每个参数向量 $\\theta$ 对应一个概率分布，参数空间 $\\Theta$ 实际上是一个**统计流形**，而不是平坦的欧几里得空间。\n",
    "\n",
    "## 2. 信息几何的观点\n",
    "\n",
    "### 2.1 统计流形\n",
    "\n",
    "在参数 $\\theta$ 附近，微小的参数变化 $d\\theta$ 会引起分布的变化。关键问题是：\n",
    "\n",
    "**如何衡量两个概率分布之间的\"距离\"？**\n",
    "\n",
    "### 2.2 KL散度：自然的距离度量\n",
    "\n",
    "两个概率分布 $p(x|\\theta)$ 和 $p(x|\\theta + d\\theta)$ 之间的KL散度：\n",
    "\n",
    "$$D_{KL}(p(x|\\theta) \\| p(x|\\theta + d\\theta)) = \\int p(x|\\theta) \\log \\frac{p(x|\\theta)}{p(x|\\theta + d\\theta)} dx$$\n",
    "\n",
    "### 2.3 二阶泰勒展开\n",
    "\n",
    "对KL散度进行二阶泰勒展开：\n",
    "$$D_{KL}(p(x|\\theta) \\| p(x|\\theta + d\\theta)) \\approx \\frac{1}{2} \\sum_{i,j} G_{ij}(\\theta) \\, d\\theta_i \\, d\\theta_j$$\n",
    "\n",
    "其中 $G_{ij}(\\theta)$ 就是**Fisher信息矩阵**：\n",
    "$$G_{ij}(\\theta) = E_{x \\sim p(x|\\theta)}\\left[\\frac{\\partial \\log p(x|\\theta)}{\\partial \\theta_i} \\frac{\\partial \\log p(x|\\theta)}{\\partial \\theta_j}\\right]$$\n",
    "\n",
    "## 3. Fisher信息矩阵的几何意义\n",
    "\n",
    "### 3.1 作为黎曼度量\n",
    "\n",
    "Fisher信息矩阵 $G(\\theta)$ 定义了统计流形上的**黎曼度量**：\n",
    "$$ds^2 = \\sum_{i,j} G_{ij}(\\theta) \\, d\\theta_i \\, d\\theta_j$$\n",
    "\n",
    "这个度量衡量了参数变化引起的\"真正的\"分布变化。\n",
    "\n",
    "### 3.2 为什么是\"自然\"的？\n",
    "\n",
    "**参数化不变性**：\n",
    "如果我们进行参数变换 $\\phi = f(\\theta)$，新参数下的Fisher矩阵满足：\n",
    "$$G_\\phi = J^T G_\\theta J$$\n",
    "\n",
    "其中 $J$ 是Jacobian矩阵。这保证了几何性质不依赖于参数化的选择。\n",
    "\n",
    "## 4. 自然梯度的数学推导\n",
    "\n",
    "### 4.1 梯度下降的约束优化观点\n",
    "\n",
    "传统梯度下降可以看作约束优化问题：\n",
    "$$\\min_{d} \\nabla L^T d \\quad \\text{subject to} \\quad \\|d\\|^2 \\leq \\epsilon^2$$\n",
    "\n",
    "解为：$d = -\\alpha \\nabla L$\n",
    "\n",
    "### 4.2 自然梯度的约束\n",
    "\n",
    "在统计流形上，应该使用Fisher度量约束：\n",
    "$$\\min_{d} \\nabla L^T d \\quad \\text{subject to} \\quad d^T G(\\theta) d \\leq \\epsilon^2$$\n",
    "\n",
    "### 4.3 拉格朗日乘数法求解\n",
    "\n",
    "构造拉格朗日函数：\n",
    "$$\\mathcal{L}(d, \\lambda) = \\nabla L^T d + \\lambda(d^T G(\\theta) d - \\epsilon^2)$$\n",
    "\n",
    "求偏导数并令其为零：\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial d} = \\nabla L + 2\\lambda G(\\theta) d = 0$$\n",
    "\n",
    "解得：\n",
    "$$d = -\\frac{1}{2\\lambda} G(\\theta)^{-1} \\nabla L$$\n",
    "\n",
    "因此，**自然梯度**为：\n",
    "$$\\tilde{\\nabla} L = G(\\theta)^{-1} \\nabla L$$\n",
    "\n",
    "### 4.4 自然梯度下降公式\n",
    "\n",
    "$$\\boxed{\\theta_{t+1} = \\theta_t - \\alpha G_t^{-1} \\nabla_\\theta L(\\theta_t)}$$\n",
    "\n",
    "## 5. 为什么这样更好？\n",
    "\n",
    "### 5.1 参数重要性的自动权衡\n",
    "\n",
    "Fisher信息矩阵的对角元素 $G_{ii}$ 反映了参数 $\\theta_i$ 的\"重要性\"：\n",
    "\n",
    "- **$G_{ii}$ 大**：参数 $\\theta_i$ 对分布影响大，应该**小心调整**\n",
    "- **$G_{ii}$ 小**：参数 $\\theta_i$ 对分布影响小，可以**大胆调整**\n",
    "\n",
    "通过 $G^{-1}$，我们自动获得了**自适应学习率**：\n",
    "$$\\alpha_i^{\\text{eff}} = \\alpha / G_{ii}$$\n",
    "\n",
    "### 5.2 几何直觉\n",
    "\n",
    "在参数空间中：\n",
    "- **平坦方向**（小特征值）→ 大步前进\n",
    "- **陡峭方向**（大特征值）→ 小步谨慎\n",
    "\n",
    "这与牛顿法的思想类似，但基于的是分布的内在几何而非损失函数的局部曲率。\n",
    "\n",
    "### 5.3 收敛性优势\n",
    "\n",
    "**定理**：对于凸损失函数，自然梯度下降具有更好的收敛性质：\n",
    "- 收敛速度不依赖于参数化\n",
    "- 在某些条件下可以达到超线性收敛\n",
    "\n",
    "## 6. 与其他方法的关系\n",
    "\n",
    "### 6.1 vs 牛顿法\n",
    "\n",
    "| 方法 | 使用矩阵 | 几何意义 | 适用场景 |\n",
    "|------|----------|----------|----------|\n",
    "| **牛顿法** | Hessian $H = \\nabla^2 L$ | 损失函数的局部曲率 | 一般优化 |\n",
    "| **自然梯度** | Fisher矩阵 $G$ | 参数空间的内在几何 | 统计学习 |\n",
    "\n",
    "### 6.2 为什么Fisher矩阵更适合统计学习？\n",
    "\n",
    "1. **稳定性**：Fisher矩阵只依赖于模型结构，不依赖于具体数据\n",
    "2. **半正定性**：总是半正定的，逆矩阵存在\n",
    "3. **几何合理性**：反映了参数空间的真实几何结构\n",
    "\n",
    "## 7. 实际应用的挑战与近似\n",
    "\n",
    "### 7.1 计算复杂度\n",
    "\n",
    "- **存储**：$O(d^2)$ 其中 $d$ 是参数数量\n",
    "- **求逆**：$O(d^3)$ 计算复杂度\n",
    "- 现代神经网络有数百万参数，直接计算不可行\n",
    "\n",
    "### 7.2 实用近似方法\n",
    "\n",
    "**1. 对角近似**：\n",
    "$$G \\approx \\text{diag}(G_{11}, G_{22}, \\ldots, G_{dd})$$\n",
    "\n",
    "**2. 块对角近似**：将参数分组，只考虑组内相关性\n",
    "\n",
    "**3. K-FAC方法**：\n",
    "$$G \\approx A \\otimes B$$\n",
    "利用Kronecker积结构降低复杂度\n",
    "\n",
    "**4. Adam等算法**：\n",
    "$$G_{ii} \\approx E[g_{i,t}^2]$$\n",
    "用梯度平方的移动平均近似Fisher矩阵对角元\n",
    "\n",
    "## 8. Adam算法的自然梯度解释\n",
    "\n",
    "Adam算法可以看作自然梯度的对角近似：\n",
    "\n",
    "$$v_t \\approx \\text{diag}(G_t)$$\n",
    "$$\\frac{m_t}{\\sqrt{v_t + \\epsilon}} \\approx G_t^{-1} \\nabla L$$\n",
    "\n",
    "这解释了为什么Adam如此有效：它在计算可行的前提下，部分捕捉了参数空间的几何结构！\n",
    "\n",
    "## 总结\n",
    "\n",
    "自然梯度使用Fisher信息矩阵的原因：\n",
    "\n",
    "1. **几何合理性**：Fisher矩阵是统计流形的自然度量\n",
    "2. **参数化不变性**：优化路径不依赖于具体参数化\n",
    "3. **自适应性**：自动权衡不同参数的重要性\n",
    "4. **理论优势**：更好的收敛性质和几何直觉\n",
    "\n",
    "现代优化算法如Adam正是这一深刻理论的实用化体现！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5011bd",
   "metadata": {},
   "source": [
    "# 历史优化算法详解：Momentum、AdaGrad、RMSprop\n",
    "\n",
    "## 1. Momentum (动量法) - 1964年\n",
    "\n",
    "### 1.1 核心思想\n",
    "\n",
    "**物理类比**：想象一个球在山坡上滚动\n",
    "- 球不仅受当前坡度影响，还保持之前的动量\n",
    "- 在平坦区域能够\"冲过去\"\n",
    "- 在震荡中能够\"平滑路径\"\n",
    "\n",
    "### 1.2 算法公式\n",
    "\n",
    "```\n",
    "v_t = β·v_{t-1} + (1-β)·∇f(θ_t)    # 动量更新\n",
    "θ_{t+1} = θ_t - α·v_t               # 参数更新\n",
    "```\n",
    "\n",
    "**参数说明**：\n",
    "- `v_t`: 动量项（速度）\n",
    "- `β`: 动量系数，通常取0.9\n",
    "- `α`: 学习率\n",
    "\n",
    "### 1.3 数学原理\n",
    "\n",
    "**指数移动平均**：\n",
    "$$v_t = (1-\\beta)\\sum_{i=1}^t \\beta^{t-i} \\nabla f(\\theta_i)$$\n",
    "\n",
    "这相当于对历史梯度进行**加权平均**，权重按指数衰减。\n",
    "\n",
    "### 1.4 解决的问题\n",
    "\n",
    "1. **加速收敛**：在一致方向上累积速度\n",
    "2. **减少震荡**：在震荡方向上相互抵消\n",
    "3. **逃离鞍点**：利用动量\"冲过\"平坦区域\n",
    "\n",
    "### 1.5 几何直觉\n",
    "\n",
    "在等高线图上：\n",
    "- **狭窄峡谷**：横向震荡被抑制，纵向加速\n",
    "- **平坦区域**：保持之前的方向和速度\n",
    "- **转弯处**：平滑过渡，避免急转弯\n",
    "\n",
    "---\n",
    "\n",
    "## 2. AdaGrad (自适应梯度) - 2011年\n",
    "\n",
    "### 2.1 核心思想\n",
    "\n",
    "**核心洞察**：不同参数应该有不同的学习率\n",
    "- 频繁更新的参数 → 较小的学习率\n",
    "- 稀少更新的参数 → 较大的学习率\n",
    "\n",
    "### 2.2 算法公式\n",
    "\n",
    "```\n",
    "G_t = G_{t-1} + ∇f(θ_t) ⊙ ∇f(θ_t)     # 累积梯度平方\n",
    "θ_{t+1} = θ_t - α/(√(G_t + ε)) ⊙ ∇f(θ_t)  # 自适应更新\n",
    "```\n",
    "\n",
    "**参数说明**：\n",
    "- `G_t`: 累积梯度平方和\n",
    "- `⊙`: 逐元素乘法\n",
    "- `ε`: 数值稳定性常数（通常为1e-8）\n",
    "\n",
    "### 2.3 数学原理\n",
    "\n",
    "**自适应学习率**：\n",
    "$$\\alpha_{t,i} = \\frac{\\alpha}{\\sqrt{G_{t,i} + \\epsilon}}$$\n",
    "\n",
    "其中 $G_{t,i} = \\sum_{s=1}^t g_{s,i}^2$ 是第i个参数的梯度平方累积。\n",
    "\n",
    "### 2.4 优势\n",
    "\n",
    "1. **自适应性**：自动调整每个参数的学习率\n",
    "2. **稀疏性友好**：对稀疏梯度效果很好\n",
    "3. **无需手动调参**：学习率自动衰减\n",
    "\n",
    "### 2.5 致命缺陷\n",
    "\n",
    "**学习率单调递减**：\n",
    "- $G_t$ 单调递增，学习率 $\\frac{\\alpha}{\\sqrt{G_t}}$ 单调递减\n",
    "- 训练后期学习率趋于0，导致**过早停止**\n",
    "- 无法从局部最优中逃脱\n",
    "\n",
    "---\n",
    "\n",
    "## 3. RMSprop (Root Mean Square prop) - 2012年\n",
    "\n",
    "### 3.1 设计目标\n",
    "\n",
    "**解决AdaGrad的问题**：\n",
    "- 保持自适应学习率的优势\n",
    "- 避免学习率过度衰减\n",
    "- 允许学习率在需要时\"恢复\"\n",
    "\n",
    "### 3.2 算法公式\n",
    "\n",
    "```\n",
    "v_t = β·v_{t-1} + (1-β)·∇f(θ_t) ⊙ ∇f(θ_t)     # 梯度平方的指数移动平均\n",
    "θ_{t+1} = θ_t - α/(√(v_t + ε)) ⊙ ∇f(θ_t)        # 自适应更新\n",
    "```\n",
    "\n",
    "**参数说明**：\n",
    "- `v_t`: 梯度平方的指数移动平均\n",
    "- `β`: 衰减因子，通常取0.9或0.99\n",
    "\n",
    "### 3.3 关键改进\n",
    "\n",
    "**指数移动平均 vs 累积求和**：\n",
    "\n",
    "**AdaGrad**：$G_t = G_{t-1} + g_t^2$ (累积，单调递增)\n",
    "\n",
    "**RMSprop**：$v_t = \\beta v_{t-1} + (1-\\beta) g_t^2$ (移动平均，可增可减)\n",
    "\n",
    "### 3.4 数学原理\n",
    "\n",
    "**权重衰减**：\n",
    "$$v_t = (1-\\beta) \\sum_{i=1}^t \\beta^{t-i} g_i^2$$\n",
    "\n",
    "- 近期梯度权重大，远期梯度权重小\n",
    "- 总权重 $\\sum_{i=1}^t (1-\\beta)\\beta^{t-i} = 1-\\beta^t \\approx 1$\n",
    "\n",
    "### 3.5 优势\n",
    "\n",
    "1. **避免学习率衰减**：旧的梯度信息会被\"遗忘\"\n",
    "2. **保持自适应性**：仍然针对不同参数调整学习率\n",
    "3. **更好的长期表现**：不会过早停止训练\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 三种方法的比较\n",
    "\n",
    "### 4.1 解决的问题对比\n",
    "\n",
    "| 算法 | 主要解决的问题 | 核心机制 |\n",
    "|------|---------------|----------|\n",
    "| **Momentum** | 震荡、收敛慢 | 历史梯度的指数移动平均 |\n",
    "| **AdaGrad** | 学习率固定 | 累积梯度平方自适应学习率 |\n",
    "| **RMSprop** | AdaGrad学习率衰减 | 梯度平方的指数移动平均 |\n",
    "\n",
    "### 4.2 数学形式对比\n",
    "\n",
    "**Momentum**：\n",
    "```\n",
    "v_t = β₁·v_{t-1} + (1-β₁)·g_t\n",
    "θ_{t+1} = θ_t - α·v_t\n",
    "```\n",
    "\n",
    "**AdaGrad**：\n",
    "```\n",
    "G_t = G_{t-1} + g_t²\n",
    "θ_{t+1} = θ_t - α·g_t/√(G_t + ε)\n",
    "```\n",
    "\n",
    "**RMSprop**：\n",
    "```\n",
    "v_t = β₂·v_{t-1} + (1-β₂)·g_t²\n",
    "θ_{t+1} = θ_t - α·g_t/√(v_t + ε)\n",
    "```\n",
    "\n",
    "### 4.3 各自的局限性\n",
    "\n",
    "**Momentum**：\n",
    "- 仍然使用固定学习率\n",
    "- 对不同参数一视同仁\n",
    "\n",
    "**AdaGrad**：\n",
    "- 学习率单调递减\n",
    "- 训练后期可能停滞\n",
    "\n",
    "**RMSprop**：\n",
    "- 缺少偏差校正\n",
    "- 在某些情况下仍可能震荡\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 通向Adam的道路\n",
    "\n",
    "### 5.1 理想的优化器\n",
    "\n",
    "结合三者优势：\n",
    "- **Momentum的方向记忆**\n",
    "- **RMSprop的自适应学习率**  \n",
    "- **更好的偏差校正**\n",
    "\n",
    "### 5.2 Adam = Momentum + RMSprop + 偏差校正\n",
    "\n",
    "```\n",
    "# 一阶矩估计 (类似Momentum)\n",
    "m_t = β₁·m_{t-1} + (1-β₁)·g_t\n",
    "\n",
    "# 二阶矩估计 (类似RMSprop)  \n",
    "v_t = β₂·v_{t-1} + (1-β₂)·g_t²\n",
    "\n",
    "# 偏差校正\n",
    "m̂_t = m_t/(1-β₁ᵗ)\n",
    "v̂_t = v_t/(1-β₂ᵗ)\n",
    "\n",
    "# 参数更新\n",
    "θ_{t+1} = θ_t - α·m̂_t/√(v̂_t + ε)\n",
    "```\n",
    "\n",
    "这就是为什么Adam被称为\"站在巨人肩膀上\"的算法！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8a4b3",
   "metadata": {},
   "source": [
    "# Adam算法公式的理论依据详解\n",
    "\n",
    "## 一、优化问题的背景\n",
    "\n",
    "### 1.1 传统梯度下降的局限性\n",
    "\n",
    "标准梯度下降法的更新规则：\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta L(\\theta_t)$$\n",
    "\n",
    "存在的问题：\n",
    "1. **学习率固定**：所有参数使用相同的学习率 $\\alpha$\n",
    "2. **收敛缓慢**：在平坦区域梯度很小，收敛极慢\n",
    "3. **震荡现象**：在狭窄峡谷中会产生剧烈震荡\n",
    "4. **鞍点问题**：容易困在鞍点附近\n",
    "\n",
    "### 1.2 解决方案的演进\n",
    "\n",
    "1. **Momentum (动量法)**：利用历史梯度信息\n",
    "2. **AdaGrad**：自适应学习率，但学习率单调递减\n",
    "3. **RMSprop**：解决AdaGrad学习率过度衰减问题\n",
    "4. **Adam**：结合Momentum和RMSprop的优点\n",
    "\n",
    "## 二、Adam算法的理论基础\n",
    "\n",
    "### 2.1 核心设计思想\n",
    "\n",
    "Adam的设计基于以下两个关键观察：\n",
    "\n",
    "1. **一阶矩估计** (First Moment Estimation)：\n",
    "   - 梯度的期望值 $E[g_t]$ 指示了最优方向\n",
    "   - 使用指数移动平均估计：$m_t \\approx E[g_t]$\n",
    "\n",
    "2. **二阶矩估计** (Second Moment Estimation)：\n",
    "   - 梯度平方的期望值 $E[g_t^2]$ 反映了梯度的方差\n",
    "   - 用于自适应调整学习率：$v_t \\approx E[g_t^2]$\n",
    "\n",
    "### 2.2 指数移动平均的数学原理\n",
    "\n",
    "#### 为什么使用指数移动平均？\n",
    "\n",
    "对于序列 $\\{g_1, g_2, \\ldots, g_t\\}$，我们希望估计其期望值。\n",
    "\n",
    "**简单平均**：\n",
    "$$\\bar{g}_t = \\frac{1}{t}\\sum_{i=1}^t g_i$$\n",
    "\n",
    "**指数移动平均**：\n",
    "$$m_t = \\beta m_{t-1} + (1-\\beta) g_t$$\n",
    "\n",
    "展开可得：\n",
    "$$m_t = (1-\\beta)\\sum_{i=1}^t \\beta^{t-i} g_i$$\n",
    "\n",
    "这相当于对历史梯度进行加权平均，权重按指数衰减：\n",
    "- 最近的梯度权重最大：$(1-\\beta)$\n",
    "- 越久远的梯度权重越小：$(1-\\beta)\\beta^{t-i}$\n",
    "\n",
    "#### 指数移动平均的优势\n",
    "\n",
    "1. **计算效率**：只需存储前一步的值，$O(1)$ 空间复杂度\n",
    "2. **自适应遗忘**：自动淡化过时的信息\n",
    "3. **平滑性**：减少噪声对估计的影响\n",
    "\n",
    "### 2.3 偏差校正的必要性\n",
    "\n",
    "#### 问题分析\n",
    "\n",
    "初始化 $m_0 = 0, v_0 = 0$ 时，在训练初期：\n",
    "\n",
    "$$m_t = (1-\\beta_1)\\sum_{i=1}^t \\beta_1^{t-i} g_i$$\n",
    "\n",
    "当 $t$ 较小时，$m_t$ 会严重偏向于 0，因为：\n",
    "$$E[m_t] = E[g] \\cdot (1-\\beta_1^t)$$\n",
    "\n",
    "如果 $E[g] \\neq 0$，则 $E[m_t] \\neq E[g]$，存在偏差。\n",
    "\n",
    "#### 偏差校正公式\n",
    "\n",
    "为了得到无偏估计，我们需要校正：\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}$$\n",
    "\n",
    "这样：\n",
    "$$E[\\hat{m}_t] = \\frac{E[m_t]}{1-\\beta_1^t} = \\frac{E[g] \\cdot (1-\\beta_1^t)}{1-\\beta_1^t} = E[g]$$\n",
    "\n",
    "同理对二阶矩：\n",
    "$$\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "\n",
    "## 三、Adam更新规则的推导\n",
    "\n",
    "### 3.1 自然梯度的视角\n",
    "\n",
    "从自然梯度下降的角度，理想的更新应该是：\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\cdot G_t^{-1} \\nabla_\\theta L(\\theta_t)$$\n",
    "\n",
    "其中 $G_t$ 是Fisher信息矩阵。但计算 $G_t^{-1}$ 代价昂贵。\n",
    "\n",
    "### 3.2 对角近似\n",
    "\n",
    "Adam使用对角矩阵近似：\n",
    "$$G_t \\approx \\text{diag}(\\hat{v}_t)$$\n",
    "\n",
    "因此更新规则变为：\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "这里：\n",
    "- $\\hat{m}_t$：一阶矩的无偏估计（方向）\n",
    "- $\\sqrt{\\hat{v}_t}$：二阶矩的平方根（自适应学习率）\n",
    "- $\\epsilon$：防止除零的小常数\n",
    "\n",
    "### 3.3 参数选择的理论依据\n",
    "\n",
    "**学习率 $\\alpha$**：\n",
    "- 通常选择 0.001\n",
    "- 控制整体的更新步长\n",
    "\n",
    "**一阶矩衰减率 $\\beta_1$**：\n",
    "- 默认 0.9，对应约 10 步的记忆\n",
    "- $\\frac{1}{1-\\beta_1} = 10$\n",
    "\n",
    "**二阶矩衰减率 $\\beta_2$**：\n",
    "- 默认 0.999，对应约 1000 步的记忆\n",
    "- $\\frac{1}{1-\\beta_2} = 1000$\n",
    "- 比 $\\beta_1$ 大是因为二阶矩变化更慢\n",
    "\n",
    "**数值稳定项 $\\epsilon$**：\n",
    "- 默认 $10^{-7}$ 或 $10^{-8}$\n",
    "- 防止分母为零\n",
    "\n",
    "## 四、收敛性分析\n",
    "\n",
    "### 4.1 收敛条件\n",
    "\n",
    "Adam算法在以下条件下收敛：\n",
    "\n",
    "1. **有界梯度**：$\\|g_t\\| \\leq G$ 对所有 $t$\n",
    "2. **有界参数**：参数序列有界\n",
    "3. **适当的学习率**：满足 $\\sum_{t=1}^\\infty \\alpha_t = \\infty$ 且 $\\sum_{t=1}^\\infty \\alpha_t^2 < \\infty$\n",
    "\n",
    "### 4.2 收敛速度\n",
    "\n",
    "在凸优化问题中，Adam的收敛速度为：\n",
    "$$E[f(\\theta_T)] - f(\\theta^*) = O\\left(\\frac{\\log T}{\\sqrt{T}}\\right)$$\n",
    "\n",
    "这与SGD的 $O(1/\\sqrt{T})$ 相当，但实际表现通常更好。\n",
    "\n",
    "## 五、Adam的变种与改进\n",
    "\n",
    "### 5.1 AdamW (Adam with Weight Decay)\n",
    "解决L2正则化在Adam中的问题：\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\left(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_t\\right)$$\n",
    "\n",
    "### 5.2 AMSGrad\n",
    "解决Adam可能不收敛的问题：\n",
    "$$\\hat{v}_t = \\max(\\hat{v}_{t-1}, v_t)$$\n",
    "\n",
    "### 5.3 AdaBound\n",
    "结合Adam和SGD的优点：\n",
    "$$\\text{lr}_t = \\text{clip}\\left(\\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon}, \\text{lower}, \\text{upper}\\right)$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
